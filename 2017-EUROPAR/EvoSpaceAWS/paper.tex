
\documentclass{llncs}
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{subfigure}
\usepackage{url}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\sloppy

\title{Random Selection of Parameters in Asynchronous Pool-Based Evolutionary Algorithms}

\author{Mario Garc\'ia-Valdez\inst{1} 
\and Ren\'e M\'arquez\inst{1} \and Juan J. Merelo Guerv\'os\inst{2} \and  Leonardo Trujillo \inst{1}}

\institute{Instituto Tecnol\'ogico de Tijuana, Tijuana BC, Mexico
\and
Universidad de Granada, Granada, Spain
\email{mario@tectijuana.edu.mx}\\
\email{renemarquezvalenzuela@gmail.com}\\
\email{jmerelo@geneura.ugr.es}\\
\email{leonardo.trujillo@tectijuana.edu.mx}}

\maketitle

\begin{abstract}
It is not always possible to set up an distributed system with
homogeneous nodes to run algorithms in a synchronous way. In grid,
cloud or volunteer setups nodes are heterogeneous, or simply are not
available at the exact same time; this is a challenge for the researcher if
their full performance is going to be actually leveraged. We are
interested in evolutionary algorithms (EAs), which evolve a population
of solutions using a mechanism inspired by biological evolution; in this field, 
several asynchronous Evolutionary Algorithms (EAs) that distribute the
evolutionary process among heterogeneous nodes have
been proposed. These algorithms make the population shared between
distributed workers  which execute the actual evolutionary
process by taking samples of the population, and replacing them in the
population pool by evolved individuals. The performance of these EAs
depends in part on the selection of parameters for the EA running in
each worker, which may include sample size, generations, mutation rate
and crossover rate along with the overall configuration. In this paper
we present a method inspired by a strategy proposed by Gong and Fukunaga for the
Island-Model which statically assigns random parameter settings to
each island in a cloud setting. Experiments were conducted in the
cloud using 2, 6 and 12 virtual machine configurations, with
both homogeneous and heterogeneous random settings using five 
test functions for single-objective optimization (Rastrigin, Griewank, De Jong, Schaffer 
and Ackley) and the OneMax binary problem. The results suggest that this approach can yield
performance improvements which are competitive with instances of the
algorithm using workers with control parameters tuned specifically for
the benchmark.

% Abstract proposed by Leonardo
% This work studies pool-based evolutionary algorithms (PEAs), that
% asynchronously distribute the evolutionary process among multiple computing nodes.
% PEAs provide a central repository or pool where the complete population is stored.
% Distributed nodes (or workers) asynchronously interact with the pool by taking a 
% sample of the population and performing a local evolutionary search on the population
% sample, then returning to the pool the newly evolved solutions.
    %%% Not all PEAs here you are talking about EvoSpace, others
    %%% you can have an island model using PEAs, and migration is throu the pool - Mario  
% Like any other EAs, the performance of the complete search will depend
% on the correct tuning of the EA parameters. In this case this problem 
% is magnified by a factor of $N$, the number of workers.
% This paper evaluates two approaches. First, to set the main parameters 
% homogeneously across all workers, based on a previous random sampling of parameter space.
% The second approach is based on the work by Gong and Fukunaga 
% proposed for the Island-Model EA called Randomized Parameter Setting Strategy,
% where parameters are set randomly for each worker and are thus heterogeneous across workers.
% Experiments were conducted in the cloud using 2, 6 and 12 virtual machine configurations, using five 
% test functions for single-objective real-valued optimization problems (Rastrigin, Griewank, De Jong, Schaffer 
% and Ackley) and the OneMax binary problem. The results suggest that the heterogeneous approach can yield
% performance improvements over the homogeneous method without requiring any previous tuning heuristic.




\keywords{Distributed Evolutionary Algorithms, Volunteer Computing,
  Cloud Computing}
\end{abstract}
\section{Introduction}
%Why Paralel
Biological evolution is an intrinsically parallel, distributed and asynchronous process, however, 
some of these features are not trivially included into standard EAs,  which are mostly coded as sequential 
and synchronous algorithms \cite{eiben}. For instance, a large body of work exists in EA parallelization, 
using multiple CPU cores, multiple nodes and GPUs \cite{}.
However, distributed and asynchronous EAs have started to become common only recently. In an effort to
exploit computing resources available from personal computers and smart-devices to massive data centers.
These resources are easily accessible through popular Internet technologies, such as cloud computing, 
peer-to-peer (P2P), and web environments. Moreover, these technologies are intended for the development 
of parallel, distributed and asynchronous systems, such that an EA developed on top of them could easily 
reap the benefits of these features.

%Pool Based
In this work, we are interested in EAs systems that follow a
pool-based approach, where the search process is conducted by a
collection of possibly heterogeneous processes that collaborate using
a shared repository or population pool. We will refer to such
algorithms as Pool-based EAs or PEAs, and highlight the fact that such
systems are intrinsically parallel, distributed and asynchronous.
The particular PEA implemented in this paper is implmented on 
the EvoSpace framework  \cite{GValdez2015}
in which distributed nodes (or workers) asynchronously interact 
with the pool by taking a sample of the population and performing 
a local evolutionary search on the population sample, then returning 
to the pool the newly evolved solutions. 
% EA Paramaters 
Like any other EAs, the performance of the complete search will depend
on the correct tuning of the EA parameters. However, settings 
must be optimized to each particular problem \cite{de2007parameter}  
giving users an additional optimization task. 
Substantial work has been focused on facilitating the burden of finding 
the most appropriate parameters settings, suggesting several strategies:
using parameters of previous studies \cite{eiben1999parameter} 
optimization by another genetic algorithm \cite{}, Dynamic adaptation \cite{}, 
self-adaptive parameters \cite{pellerin2004self}, hybrid approaches and so on.
% Worst on many workers
In heterogeneous and distributed PEAs like EvoSpace, this problem can 
be magnified by a factor of $N$, the number of workers, because each one
can be treated as an independent EA with local parameters. Moreover, there
are additional parameters particular to the EvoSpace framework, 
for instance the size of the samples.
% Dynamic adaptation
Several works propose the use of adaptive crossover and mutation probabilities
that depend on the current genetic diversity of the population \cite{pellerin2004self},
arguing that modifying these parameters can prevent the rapid convergence of the 
algorithm. 
% Explore and Exploit 
% Is this pertinent?
A common dynamic heuristic is to explore the solution space first and then exploit
when a local optima is found and repeat if necessary.  
% Random Patameters
Having multiple EAs running in parallel with different parameters could be 
equivalent to performing a dynamic adaptation, having some workers exploring
and others exploiting at the same time. 
% Evaluate in EvoSpace
In the study presented here, a recent approach called Randomized Parameter
Setting Strategy (RPSS) \cite{fuku1,fuku2} is applied to EvoSpace and tested on 
several benchmark problems, extending the work presented in \cite{garcia2014randomized} 
in which only trap functions were applied.
The idea behind RPSS is that in a distributed EA, parametrization may be
completely skipped for a successful search, with research showing that when the 
number of distributed process is large enough, algorithm parameters can be set 
randomly and still achieve good overall results. However, work on RPSS has 
only focused on the well-known Island Model for EAs, a distributed but synchronous system.

The remainder of the paper proceeds as follows.  Section \ref{sec:work} 
reviews related work. Afterwards, Section \ref{sec:evo} briefly describes the
proposed EvoSpace cloud implementation, the experimental work is presented in 
Section \ref{sec:experiments}. Finally, a summary and concluding remarks are in
Section \ref{sec:conclusions}.

\section{Related Work}
\label{sec:work}
% Dynamic adaptation
% Random Patameters

\section{EvoSpace Cloud Implementation}
\label{sec:evo}
% Architecture Components
% Random Patameters
\begin{figure*}[t]
    \centering
        \includegraphics[width=10cm]{img/evospace-aws.png}
    \caption{Main components and data-flow in the cloud version of EvoSpace. }
    \label{fig:evospace}
\end{figure*}

\section{Experiments}
 \ref{sec:experiments}

The goal of this work is to determine if a random static parametrization for each of the $n$ EvoWorkers 
collaborating on a given run could achieve competitive results against an homogeneous tuned parametrization
\cite{fuku1,fuku2,garcia2014randomized}. The parameters considered to be randomly set for each EvoWorker 
where the crossover and mutation probabilities with a valid range of $[0,1]$. These parameters where 
selected because they effectively control which types of moves in the search space are
applied, and are related to the amount of exploration and exploitation that is carried out \cite{}.
The other parameters and design choices used by EvoSpace and the EvoWorkers in our experiments are given in 
Table \ref{tab:params}. Additionally, for the binary OneMax problem bit-flip mutation
is used with an independent probability for each attribute to be flipped (indpb) of $0.05$, 
a two point crossover and a tournament selection of size 3. For the real-valued problems a Gaussian
mutation was applied with mu=$0$, sigma=$0.2$, and indpb=$0.05$; two point crossover and
a tournament selection of three. These values are kept static to measure only the effect of the 
parameters mentioned above.
    % Why do these parameters don't change?
    % Explained - Mario 


\begin{table}[!t]
\caption{Valid ranges for each EvoWorker parameters. One Max}
\label{tab:params}
\centering
\begin{tabular}{|l|c|c|c|c|c|c| }
\hline
\textbf{Parameter} & \multicolumn{3}{|c|}{OneMax} & \multicolumn{3}{|c|}{Test Functions} \\
\hline
Number of Workers & 2 & 6 & 12 & 2 & 6 & 12\\
\hline
\hline
Population Size & 200 & 280 & 620 & 120 & 150 & 150\\
\hline
Sample Size & 40 & 40 & 40 & 40 & 20 & 10\\
\hline
Maximum Samples & 30 & 40 & 40 & 200 & 100 & 50\\
\hline
Local Generations & 30 & 30 & 30 & 100 & 100 & 100\\
\hline
\end{tabular}
\end{table}

To measure the effectiveness of RPSS in EvoSpace two parametrization strategies are compared, 
similar to what is done in \cite{fuku1,fuku2,garcia2014randomized}. First, we consider the approach of setting all 
of the EvoWorker parameters homogeneously. In order to tune the homogeneous parameters,
we select the best configuration from 100 random parameterizations. 
Each of these random parametrization are evaluated based on their average performance over 10 
independent runs for each problem.
Hereafter, we refer to this method as the {\em homogeneous} configuration. Second, for RPSS the parametrizations
are not tuned, they are randomly generated for each EvoWorker, set independently at the beginning of each run.
Hereafter, we call this approach the {\em heterogeneous} configuration. Finally, we perform 30 independent runs
with both methods and report the average number of evaluations required by each strategy to find the
global optimum solution.

The algorithms are first evaluated using the OneMax (or BitCounting) problem proposed by 
Schaffer and Eshelman \cite{SE91}, this is a simple problem consisting in maximizing the number 
of ones of a bitstring. For the current experiments a 128 bit string was used. Since this
problem is not computationally demanding, more experiments where conducted for the tuning phase, 
tuning the parameters using 2, 6, and 12 EvoWorkers. Then, following \cite{fuku1}, 
five standard real-valued single objective optimization problems 
are used, these are the Rastrigin, Griewank, De Jong, Schaffer  and Ackley functions, 
with the number of dimensions set to 100 in each case. For these problems we use a real-valued vector
representation for each individual.


\subsection{OneMax Results}

In this section, results from the OneMax experiments are discussed. As mentioned before, the first
step is to simulate the manual tuning of the parameters by running 100 experiments with random 
parameters and then select the best configuration. As the OneMax problem is solved by almost 
all configurations, the best is the one requiring the least time to finish. Figure~\ref{fig:effort} 
shows the time required for different numbers of EvoWorkers. It can be observed that even with an 
homogeneous configuration as the number of workers increases so does
the number of fast solutions.
A reason for this could be that the solution to the problem depends on the number of evaluations, and 
as the number of workers increases so does the number of evaluations
per unit of time, which implies
that the selection of a good configuration could be found with less experiments for higher number 
of workers.


In Figure~\ref{fig:comp-onemax} the results of 30 runs comparing
against the RPSS approach is shown. %What is RPSS? - JJ
        %It will be explained in the Intro is the Random Parameter Selection Strategy - Mario
It can be observed that as the number of EvoWorkers increases the median of the time decreases, but
in this case the best of the heterogeneous solutions (12 workers) is only better than the worst homogeneous
solution.  % Explain - JJ
           % To do - Mario 

\begin{figure*}[b]
    \centering
    \subfigure [2 workers]
    {
        \includegraphics[width=3in]{img/2w_onemax_100_box.png}
    }
    \subfigure  [6 workers]
    {
        \includegraphics[width=3in]{img/6w_onemax_100_box.png}
    }
    \subfigure  [12 workers]
    {
        \includegraphics[width=3in]{img/12w_onemax_100_box.png}
    }

    \caption{100 experiments with random parameters for the 128 Bit OneMax problem.
    Experiments are ranked by the mean time to solution of 5 runs, with   
    (a) 2 workers, (b) 6 workers, and (c) 12 workers.}
    \label{fig:effort}
\end{figure*}



\begin{figure*}[t]
    \centering
        \includegraphics[width=10cm]{img/one_max_comp.png}
    \caption{Comparison of 30 runs of the 128 Bit OneMax problem. 
    Box-plot of the number of evaluations needed for solution, with a 2, 6 and 12 workers
    homogeneous configuration on the left side, and Heterogeneous configuration on the
    right side of each.
    }
    \label{fig:comp-onemax}
\end{figure*}



\subsection{ Single-Objective Optimization Test Functions}

The same steps as the previous section were followed when comparing the five real valued 
optimization problems of this section. For brevity only two representative functions are discussed
with figures, and summary of all the results are shown in Table~\ref{fig:summary}. As before
the first step is to tune the parameters of the 
asynchronous GA algorithm. In Figure \ref{fig:griewank} the results of the tuning 
phase for 6 and 12 workers for the Griewank function are shown. For 12 workers
results are not as flat as before, with times comparable to the 6 worker 
configuration in the best case.

In order to compare both configurations in Figure \ref{fig:griewank-evals} the number of evaluations 
needed for solution are presented. In this experiment, the heterogeneous solution is better than the
tuned homogeneous parametrization. But in the case of results shown in Figure~\ref{fig:schaffer} 
the homogeneous and heterogeneous configurations yield similar results, 
only with a slight advantage
for the heterogeneous configuration with 12 EvoWorkers. In the summary shown in
Table~\ref{fig:summary} two extreme cases are found the Ackley and Rastrigin functions, 
with results leaning
towards the heterogeneous and homogeneous configurations respectively. On the other hand, the De Jong
function also favors the heterogeneous configurations. 



\begin{figure*}[b]
    \centering
    \subfigure  [6 workers]
    {
        \includegraphics[width=2.2in]{img/6w_griewank_100_box.png}
    }
    \subfigure  [12 workers]
    {
        \includegraphics[width=2.2in]{img/12w_griewank_100_box.png}
    }

    \caption{100 experiments with random parameters for the 128 Bit Griewank 
    single-objective optimization test function. Experiments are ranked by 
    the mean time to solution of 5 runs, with (a) 6 workers, and (b) 12 workers.}
    \label{fig:griewank}
\end{figure*}


\begin{figure*}[b]
    \centering
    \subfigure  [Homogeneous]
    {
        \includegraphics[width=2.2in]{img/griewank_evals_homo.png}
    }
    \subfigure  [Heterogeneous]
    {
        \includegraphics[width=2.2in]{img/griewank_evals_hetereo.png}
    }
      \caption{30 runs of the 128 dimension Griewank single-objective optimization test function. 
    Box-plot of the number of evaluations needed for solution, with an (a) Homogeneous configuration, and (b) Heterogeneous configuration.}
    \label{fig:griewank-evals}
\end{figure*}

\begin{figure*}[b]
    \centering
    \subfigure  [Homogeneous]
    {
        \includegraphics[width=2.2in]{img/schaffer_evals_homo.png}
    }
    \subfigure  [Heterogeneous]
    {
        \includegraphics[width=2.2in]{img/schaffer_evals_hetereo.png}
    }
 

    \caption{30 runs of the 128 dimension Schaffer single-objective optimization test function. 
    Box-plot of the number of evaluations needed for solution, with an (a) Homogeneous configuration, and (b) Heterogeneous configuration.}
    \label{fig:schaffer}
\end{figure*}

\begin{figure*}[t]
    \centering
        \includegraphics[width=10cm]{img/table.png}
    \caption{Summary table of results. }
    \label{fig:summary}
\end{figure*}

\bibliographystyle{abbrv}
\bibliography{../../bib/biblio,../../bib/evospace-i,../../bib/parameters}

\end{document}
