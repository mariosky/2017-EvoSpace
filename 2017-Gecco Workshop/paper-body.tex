\section{Introduction}
% Main Ideas:
%Why Paralel
A large body of work exists on the parallelization of EAs,
with techniques leveraging multiple CPU cores, many computing nodes, 
and GPUs \cite{muhlenbein1989parallel,cantu2000efficient,hofmann2013performance}. 
However, asynchronous EAs
\cite{Jini:FEA2000,alba2001analyzing,Jini:FEA2000,jj:2008:PPSN} have
started to become common only relatively
recently, in an effort to exploit computing resources available
through different Internet technologies, including cloud. In this work, we are
interested in those EAs following a pool-based approach,
where a collection of heterogeneous worker processes 
carry out population search tasks by collaborating through a shared
individual 
repository or population pool. We will refer to such algorithms 
as {\em Pool-based EAs} or PEAs, and highlight the fact that 
such systems are intrinsically parallel, distributed and asynchronous.

Pool-EAs differ from the closely related island model, 
mainly with regards to the responsibilities assigned to 
the server. When there is a server in the island model, it is 
responsible for the interaction and synchronization of 
all the populations.  In Pool-EAs, on the other hand, the population repository only 
receives stateless requests from isolated workers 
or clients. In this way, Pool-EAs are capable of using and leveraging an 
ad-hoc and ephemeral collaboration of computing resources. 

The platform presented in this paper is a new implementation 
of the EvoSpace model \cite{GValdez2015} in which 
workers asynchronously interact with the population 
pool by taking samples of the population 
to perform a local evolutionary search on the samples, 
to then return newly evolved solutions back to the pool. This is a
particular instance of a pool-based EA, which, as long as there is a
common population pool, leaves every other detail to particular
implementations. Other PEAs, for instance, might return only one
individual to the pool, or use it as a read-only resource, not
returning any member to it; the {\em frequency} with which resources
are taken or returned to the population are not set by the model
either, leaving it as a implementation or model-specific parameter. % I am giving here a general overview of PEAs, maybe it should be moved up with a few references? - JJ

The previous version % anonymized reference
was implemented using CherryPy, a basic HTTP 
server written in Python. This new version uses Node.js, an 
event-driven interpreter with a built-in event loop capable of
asynchronous I/O \cite{tilkov2010node}, that is 
running on the JavaSacript V8 engine. Node.js is used 
to optimize throughput and scalability of the server. %Actually you
                                %are using express, I think. You
                                %should talk about that - JJ
Additionaly to the  increased performance this version 
adds new functionality: In the former version workers could only
ask for random samples of a particular size, now clients 
can retrieve objects from the server ordered by a score. 
Designers can use this functionality to implement 
asynchronous versions of the Island Model or to force 
the retrieval of different objects in every request 
resembling a circular queue. Instead of using the JSON-RPC 
protocol the server functionality is now exposed as a RESTful 
Web Service. The server now keeps a log of the work performed 
by workers: The number of evaluations, the best solution in each 
generation (or iteration), parameters and algorithm used among others.
This log can later be used to compare the performance of 
the algorithm against others, for instance against 
algorithms using the COCO (COmparing Continuous Optimisers)
platform \cite{hansen2016coco}.
The aim of the {\sf evospace-js} software is to provide 
researchers with a high performance platform in which 
they can execute pool-based algorithms using heterogeneous workers. 

The remainder of the paper proceeds as follows. Section \ref{sec:work} 
reviews related work. Afterwards, Section \ref{sec:evo} describes the
proposed EvoSpace implementation, the experimental work is presented in 
Section \ref{sec:experiments}. Finally, a summary and 
concluding remarks are given in Section \ref{sec:conclusions}.


\section{Related Work}
\label{sec:work}
There are two important practical issues faced by many EA and other
optimization systems, namely the size of the parameter 
space and the high computational cost when it is compared with
mathematical programming or numerical techniques. % I guess because
% they are deterministic, right? - JJ
Concerning the latter, one approach to mitigate this issue is to use parallel or 
distributed implementations \cite{cantu-paz:migration-policies,duda2013gpu}.
For instance, Fern\'andez et al. \cite{nc} % articulo Paco, Gustavo y Leo publucado en Natural Computing}
use the well-known Berkeley Open Infrastructure for Network Computing (BOINC) to distribute EA runs across a
heterogeneous network of volunteer computers using virtual machines. Another recent example is 
found in the FlexGP system developed by Sherry et al. \cite{sherry2012flex}. FlexGP is probably the first large scale GP system 
that runs on the cloud, using an Island model approach and implemented over Amazon EC2 with a 
socket-based client-server architecture. There is a considerable
improvement and scalability in this approach, but this scalability has
a cost which is proportionally much smaller than installing a
permanent infraestructure, but onerous nonetheless. % Maybe take some
                                % figures from it - JJ

In general, all the techniques and implementations mentioned above
rely on more or less {\em traditional} parallel or distributed
evolutionary algorithms, using {\em farming} for offloading
evaluations to ephemeral resources or using more traditional
island-based models in distributed or cloud-based resources. However,
there is another approach to distributed EAs: the so called pool-based
architecture \cite{talukdar1998asynchronous,pool:ga,de1991genetic}. In general, a 
pool-based system employs a central repository (real or virtual)
% What's the difference? - JJ
where the evolving population, or a part of it, is stored. 
Distributed clients interact with the pool, performing some or all of the basic EA processes 
(selection, genetic operators, survival), but these clients join the
search by just using an API, and quit by simply not doing it any
more. Clients are not considered reliable in any way, and the
threshold to join the pool and perform an operation is kept as low as
possible. 
A representative work of this approach 
is that by Merelo et al. \cite{agajaj} implementing a JavaScript based PEA that distributes 
the evolutionary process over the web, providing the added advantage of not requiring the 
installation of additional software in each computing node.  Other similar cloud-based solutions 
are based on a global queue of tasks and a Map-Reduce implementation which normally handles failures 
by the re-execution of  tasks \cite{fazenda2012,di2013towards,FlexGP}. Using the BOINC 
volunteer platform  Smaoui et al. \cite{FekiNG09} uses work units that consist of a fitness 
evaluation task and multiple replicas  were produced and sent to different clients.

While using a distributed framework can ease the computational cost, it can also exacerbate the first issue mentioned above;
i.e., it increases the size of the algorithm parameter space, which makes parameter tuning a more difficult task.
The issue of optimal parametrization of EAs is a widely studied subject \cite{de2007parameter}, 
with many approaches in literature. For instance, one of the most successful approaches 
is the F-Racing and iterative F-Racing techniques \cite{lopez2011irace}. 
However, while such algorithms can find high performance parametrization, 
they require additional computational effort which can be too expensive in some applications
(even if they are more efficient than an exhaustive search).

\begin{figure}[!t]
    \centering
        \includegraphics[width=2.5in]{img/classes.png}
    \caption{ UML Class diagram of the Population and Individual classes.}
    \label{fig:classes}
\end{figure}
%
\begin{figure*}[!t]
    \centering
        \includegraphics[width=5.4in]{img/evospace-js.png}
    \caption{ Stack diagram of the {\sf evospace-js} framework components.}
    \label{fig:stack}
\end{figure*}

\section{{\sf evospace-js} Implementation}
\label{sec:evo}
The main components of the EvoSpace framework are: the {\sf evospace-js} 
population container, remote clients called EvoWorkers.
Each of these components are defined in the following subsections.

\subsection{{\sf evospace-js}} % Maybe qualify this, it's got the same
                               % name as above. Population container?
                               % ~ JJ
 \label{sec:evospace}
The {\sf evospace-js} server provides a collection of REST methods  
to operate over a set of objects $ES$, which can be seen as the 
population. Multiple populations can be created and are 
distinguished by their name. Objects in each $ES$ 
can be selected, removed and replaced through the 
following endpoints:
\begin{enumerate}
    \item {\bf population\_name/initialize} 
    This is a {\tt POST} request used to create a new population.
    \item {\bf population\_name/individual} 
    This is a {\tt POST} request used to create and add a new object
    to a population. The object is defined in a JSON format, 
    and there is no restriction on its structure, only 
    the following properties are required: ``id'' this is an 
    integer and is generated if not present, ``fitness'' also defined 
    as a JSON object, the structure been specific to each application, 
    and finally a ``chromosome'' property again defined as
    a JavaScript object giving the internal representation of 
    the solution, by default a it defined as list of objects. 
    There is also an optional integer property called 
    ``score'' used when objects are going to be retieved in a certain order.
    \item {\bf population\_name/sample/n}
    This is a {\tt GET}  request used to take from the population a 
    sample of {\bf n} objects. These objects are removed from the 
    population and are no longer available
    to other requests until and only if they are put back. 
    Objects can be returned to the population 
    either by a {\tt PUT} sample request called from the same 
    client or by a Respawn request. The reason for 
    this is to avoid concurrently write conflicts and duplication of work.
    \item {\bf population\_name/sample}
    This is a {\tt POST} request used to put back a sample to the population.
    The new sample is sent in the request body as a JSON object. 
    If the client created new objects or 
    changed their original state, these objects replace the originals. 
    \item {\bf population\_name/respawn}
    This is a {\tt POST} request used to put back {\bf n} samples to their 
    original state. The number of samples is sent in the request body. 
\end{enumerate}
There are other secondary REST endpoints used to: select all objects in a 
population, select objects with scores with in a range, read the 
top {\tt n} objects according to a score and read the number of
objects currently on the population.      

The above methods were implemented first as JavaScript library 
with two classes: {\tt Individual} and {\tt Population} depicted 
in Figure~\ref{fig:classes} with calls to the Redis memory store 
through the {\bf ioredis} 
asynchronous library. In order to expose the library as a 
REST Web service endpoints were implemented using the Express HTTP framework. 
An optional dashboard type application, can be used to inspect 
the populations currently available on the server.


When a worker is putting back a sample, it can send 
an additional property called {\tt benchmark\_data }
to send supplementary information about the execution 
of the experiment.  This data can later be used to 
benchmark the performance of the algorithm. This 
data is again stored in redis as an ordered list, 
keeping a log for each experiment. Currently the JSON 
benchmark data structure contains the following details 
to later be used by the COCO platform: the algorithm identifier, 
parameters used, name , dimension, instance and optimal value 
of the function that is been optimized, worker and experiment 
identifiers and finally a list of details of each iteration 
or generation of the local execution. The details include:
the best solution and the function value, and the number of
function evaluations requiered. Depending on the application
other data could be recorded. The source code for the {\sf evospace-js}
server is in the following Github repository: 
\url{https://github.com/mariosky/evospace-js}.


\subsection{EvoWorkers}
\label{sec:evoworkers}
As we mentioned earlier, EvoWorkers are independent of 
the population store, and developers can implement them 
in any language that supports HTTP requests. To develop an 
EvoWorker, a programmer could just write the code needed to 
take a sample of the population and use this sample to 
replace the initial population of a local algorithm. 
Then after a certain number of iterations return the 
current population back to the server.

In this work, EvoWorkers were implemented in Python 
using two open source libraries of nature inspired optimization 
metaheuristics:  DEAP and EvoloPy. For each algorithm a 
python script was responsible for the initialization using 
the required parameters and setting up the initial population, 
then after some iterations, the current population and 
benchmark data is sent back to the server. EvoWorker 
scripts can run in Docker containers, by receiving 
the initial parameters as environment variables, and 
the script ends when it reaches a maximum number of samples.
The source code for the Python EvoWorkers
proposed in this work are in the following GitHub repository: 
\url{https://github.com/mariosky/EvoWorker}.

\section{Experiments}
 \label{sec:experiments}
As a case study, a simple hybrid algorithm consisting 
of PSO and GA EvoWorkers was used to run a benchmark that 
included the first three functions found in the COCO platform:  
Sphere (F1), Ellipsoid (F2), and Rastrigin (F3). The objective 
of the algorithm is not to be a competitive solution for 
the optimization benchmark, as the intention is to test 
the software functionality. After the 
execution, a script processed the logs and generated the files 
needed by the COCO platform post-processing scripts. 

A requirement of the COCO platform is that it needs 
to inspect each function evaluation to keep the log required 
to analyze the execution. The logging code maintains 
a sequential record of the number function calls. 
This exact order is not practical to keep in an asynchronous 
execution as many workers are calling the function at the 
same time. For this reason, the granularity of the number of 
function evaluations and their order is kept at the 
sample-iteration level. As we mentioned earlier, each worker 
returns a record with benchmark data, with the number of 
evaluations performed in each iteration.

Is not practical to track the exact sequential number 
of function evaluation in an asynchronous execution, 
as many workers could be calling the function at the same 
time. For this reason, the granularity of the number of 
function evaluations and their order was kept at the sample 
and iteration level. As we mentioned earlier, each worker returns
a record with benchmark data containing the number of 
evaluations performed in each iteration. The order and number 
of function calls were given by the order in which 
the server received the samples and sequence of iterations.
A After each iteration, the best solution was considered 
to happen at the last function evaluation. Is important to
notice that EvoWorkers run the algorithm only for a small 
number of iterations and with a relatively small sample of 
the population. For instance, for the COCO benchmark 
presented in the case study the maximum number of 
function evaluations in a single iteration was 200. 
  

\subsection{Set-up}
\label{sec:evoworkers}

A script was responsible for creating the EvoWorker 
containers and running the benchmark. The three functions 
F1 to F3 were tested with 15 instances for each of 
the following dimensions:  2, 3, 5, 10, 20, and 40. 
The maximum number of function evaluations was set to $10^5*dim$. 
In order to maintain the required number of function 
evaluations the following parameters were set to each dimension.

\begin{table}
  \small
  \caption{EvoWorker setup parameters}
  \label{tab:params} 
  \centering
  \small
  \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline\noalign\\
    Dimension & 2 & 3 & 5 & 10 & 20 & 40\\\hline
    Iterations per Sample  & 50 & 50 & 50 & 50 & 50 & 50\\ \hline
    Sample Size  & 100 & 100 & 100 & 200 & 200 & 200 \\ \hline
    Samples per Worker & 20 & 30 & 25 & 25 & 25 & 25  \\ \hline
    PSO Workers & 1 & 1 & 2 & 2 & 4 & 8  \\ \hline
  \end{tabular}
\end{table}

\begin{table}
  \small
  \caption{ GA EvoWorker Parameters }
  \label{tab:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline\noalign\\
      Search space &  $[-4,4]^{D}$ \\ \hline
      Selection & Tournament size=12\\ \hline
      Mutation &  \\ \hline

  \end{tabular}
\end{table}



%TO DO: TABLE
%Data in https://docs.google.com/spreadsheets/d/123s_p6ABtyT2wdifxjuHVGuXjTZECmfzOddxUTydbI8/edit?usp=sharing



\subsection{Results}
\label{sec:results}
After the experiment was executed a script generated 
the folders needed by the COCO post-processing scripts.

\begin{figure*}[h!t]
    \centering
        \includegraphics[width=5in]{img/Sphere.pdf}
    \caption{ TO DO: Caption.}
    \label{fig:sphere}
\end{figure*}


\begin{figure*}[h!t]
    \centering
        \includegraphics[width=5in]{img/Ellipsoid.pdf}
    \caption{ TO DO: Caption.}
    \label{fig:ellipsoid}
\end{figure*}

\begin{figure*}[h!t]
    \centering
        \includegraphics[width=5in]{img/Rastrigin.pdf}
    \caption{ TO DO: Caption.}
    \label{fig:rastrigin}
\end{figure*}


\section{Conclusions and Further Work}
\label{sec:conclusions}

% To be written

Future lines of work will focus on using other EA or meta-heuristic techniques, 
such as genetic programming or particle swarm optimization for having 
workers that are heterogeneous in more than one sense. RPSS could be
used in those cases where each algorithm has different sets of
parameters, but also to randomly select the technique used in each
node. Another interesting line of work is the dynamic adaptation of
parameters by measuring the diversity of each worker or returned
sample. This could be specially useful in cases where the random
parametrization technique seems to achieve bad results. 

\begin{acks}
This work has been supported in part by:  Ministerio espa\~{n}ol de
Econom\'{\i}a y Competitividad under project TIN2014-56494-C4-3-P
(UGR-EPHEMECH).
\end{acks}
