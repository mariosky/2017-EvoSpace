\section{Introduction}
% Main Ideas:
%Why Paralel
A large body of work exists on the parallelization of EAs,
with techniques leveraging multiple CPU cores, many computing nodes, 
and GPUs \cite{muhlenbein1989parallel,cantu2000efficient,hofmann2013performance}. 
However, asynchronous EAs
\cite{Jini:FEA2000,alba2001analyzing,EvoStar2014:jsEO,jj:2008:PPSN} have
started to become common only relatively
recently, in an effort to exploit computing resources available
through different Internet technologies, including cloud. In this work, we are
interested in those asynchronous EAs following an approach that uses a
shared {\em pool} of individuals with a collection of heterogeneous worker processes 
carry out population search tasks by collaborating through this pool. We will refer to such algorithms 
as {\em Pool-based EAs} or PEAs, and highlight the fact that 
such systems are intrinsically parallel, distributed and asynchronous.

Pool-EAs differ from the  Island Model
mainly with regards to the responsibilities assigned to 
the server. When there is a server in the island model, it is 
responsible for the interaction and synchronization of 
all the populations, and this server might have a possibly ephemeral
{\em pool} of migrants in the process of being moved from one island
to another.  In Pool-EAs, on the other hand, the population repository only 
receives stateless requests from isolated workers 
or clients. In this way, Pool-EAs are capable of using and leveraging an 
ad-hoc and ephemeral collaboration of computing resources. 

The platform presented in this paper is a new implementation 
of the EvoSpace model \cite{GValdez2015} in which 
workers asynchronously interact with the population 
pool in the following way. Following their own schedule, EvoWorkers
request samples of the population from the pool and perform a local
evolutionary search using them as initial population, which after a
number of iterations is returned to the pool. The sample size and the
number of iterations this cycle lasts depends on the implementation
and is actually not so important for the overall performance of the
algorithm. Sample size has got more to do with protocol overhead: the
bigger the population, the higher proportion of
payload-to-total-packet-size is achieved. The number of iterations of
every cycle is also mainly an implementation detail: the shorter the
cycle, the higher the load on the server. The actual parameters used
in this paper are listed in Table \ref{tab:params}.
This is a
particular instance of a Pool-based EA, which, as long as there is a
common population pool, leaves every other detail to particular
implementations. Other PEAs, for instance, might return only one
individual to the pool, or use it as a read-only resource, not
returning any member to it; the {\em frequency} with which resources
are taken or returned to the population are not set by the model
either, leaving it as a implementation or model-specific parameter. % I am giving here a general overview of PEAs, maybe it should be moved up with a few references? - JJ

The previous version \cite{DBLP:journals/grid/ValdezTGVO15}
was implemented using CherryPy, a basic HTTP 
server written in Python. This new version uses Node.js, an 
event-driven interpreter with a built-in event loop capable of
asynchronous I/O \cite{tilkov2010node}, that is 
running on the JavaScript V8 engine. Node.js is used 
to optimize throughput and scalability of the server. %Actually you
                                %are using express, I think. You
                                %should talk about that - JJ
                                % Ok, I will later when explaining the server 
                                % in detail - Mario 

Additionally to the increased performance this version 
adds new functionality: In the former version workers could only
ask for random samples of a particular size, now clients 
can retrieve objects from the server ordered by a score. 
Designers can use this functionality to implement 
asynchronous versions of the island model or to force 
the retrieval of different objects in every request 
resembling a circular queue. Instead of using the JSON-RPC 
protocol the server functionality is now exposed as a RESTful 
Web Service. The server now keeps a log of the work performed 
by workers: The number of evaluations, the best solution in each 
generation (or iteration), parameters and algorithm used among others.
This log can later be used to compare the performance of 
the algorithm against others, for instance against 
algorithms using the COCO (COmparing Continuous Optimisers)
platform \cite{hansen2016coco}.
The aim of the {\sf evospace-js} software is to provide 
researchers with a high performance platform in which 
they can execute pool-based algorithms using heterogeneous workers. 

The remainder of the paper proceeds as follows. Section \ref{sec:work} 
reviews related work. Afterwards, Section \ref{sec:evo} describes the
proposed EvoSpace implementation, the experimental work is presented in 
Section \ref{sec:experiments}. Finally, a summary and 
concluding remarks are given in Section \ref{sec:conclusions}.


\section{Related Work}
\label{sec:work}
There are two important practical issues faced by many EA and other
optimization systems, namely the size of the parameter 
space and the high computational cost when it is compared with
mathematical programming or numerical techniques. % I guess because
% they are deterministic, right? - JJ
% Yes - Mario
Concerning the latter, one approach to mitigate this issue is to use parallel or 
distributed implementations \cite{cantu-paz:migration-policies,duda2013gpu}.
For instance, Fern\'andez et al. \cite{nc} % articulo Paco, Gustavo y Leo publucado en Natural Computing}
use the well-known Berkeley Open Infrastructure for Network Computing (BOINC) to distribute EA runs across a
heterogeneous network of volunteer computers using virtual machines. Another recent example is 
found in the FlexGP system developed by Sherry et al. \cite{sherry2012flex}. FlexGP is probably the first large scale GP system 
that runs on the cloud, using an island model approach and implemented over Amazon EC2 with a 
socket-based client-server architecture. There is a considerable
improvement in performance and scalability in this approach, but this scalability has
a cost which is proportionally much smaller than installing a
permanent infrastructure, but onerous nonetheless. % Maybe take some
                                % figures from it - JJ

In general, all the techniques and implementations mentioned above
rely on more or less {\em traditional} parallel or distributed
evolutionary algorithms, using {\em farming} for offloading
evaluations to ephemeral resources or using more traditional
island-based models in distributed or cloud-based resources. However,
there is another approach to distributed EAs: the so called Pool-based
architecture \cite{talukdar1998asynchronous,pool:ga,de1991genetic}. In general, a 
Pool-based system employs a central repository 
% What's the difference? - JJ
% Is central but it could be internally distributed or a cluster
% Im taking it of because is an implemantation detail - M 
where the evolving population, or a part of it, is stored. 
Distributed clients interact with the pool, performing some or all of the basic EA processes 
(selection, genetic operators, survival), but these clients join the
search by just using an API, and quit by simply not doing it any
more. Clients are not considered reliable in any way, and the
threshold to join the pool and perform an operation is kept as low as
possible. 
A representative work of this approach 
is that by Merelo et al. \cite{agajaj} implementing a JavaScript based PEA that distributes 
the evolutionary process over the web, providing the added advantage of not requiring the 
installation of additional software in each computing node.  Other similar cloud-based solutions 
are based on a global queue of tasks and a Map-Reduce implementation which normally handles failures 
by the re-execution of  tasks \cite{fazenda2012,di2013towards,FlexGP}. Using the BOINC 
volunteer platform  Smaoui et al. \cite{FekiNG09} uses work units that consist of a fitness 
evaluation task and multiple replicas  were produced and sent to different clients.

While using a distributed framework can ease the computational cost, 
it can also exacerbate the first issue mentioned above;
i.e., it increases the size of the algorithm parameter space, which makes parameter tuning a more difficult task.
The issue of optimal parametrization of EAs is a widely studied subject \cite{de2007parameter}, 
with many approaches in literature. For instance, one of the most successful approaches 
is the F-Racing and iterative F-Racing techniques \cite{lopez2011irace}. 
However, while such algorithms can find high performance parametrization, 
they require additional computational effort which can be too expensive in some applications
(even if they are more efficient than an exhaustive search).

\begin{figure}[!t]
    \centering
        \includegraphics[width=2.5in]{img/classes.png}
    \caption{ UML Class diagram of the Population and Individual classes.}
    \label{fig:classes}
\end{figure}
%
\begin{figure*}[!t]
    \centering
        \includegraphics[width=5.4in]{img/evospace-js.png}
    \caption{ Stack diagram of the {\sf evospace-js} framework components.}
    \label{fig:stack}
\end{figure*}

\section{{\sf evospace-js} Implementation}
\label{sec:evo}
The main components of the EvoSpace framework are: the {\sf evospace-js} 
population repository, remote clients called EvoWorkers.
Each of these components are defined in the following subsections.

\subsection{{\sf evospace-js} Population Repository} 
\label{sec:evospace}

The {\sf evospace-js} server provides a collection of REST methods  
to operate over a set of objects $ES$, which can be seen as the 
population. Multiple populations can be created and are 
distinguished by their identifier $ES_id$. Objects in each $ES_id$ 
can be selected, removed or replaced through the 
following endpoints:
\begin{enumerate}
    \item {\bf population\_name/initialize} 
    This is a {\tt POST} request used to create a new population.
    \item {\bf population\_name/individual} 
    This is a {\tt POST} request used to create and add a new object
    to a population. The object is defined in a JSON format, 
    and there is no restriction on its structure, only 
    the following properties are required: ``id'' this is an 
    integer and is generated if not present, ``fitness'' also defined 
    as a JSON object, the structure been specific to each application, 
    and finally a ``chromosome'' property again defined as
    a JavaScript object giving the internal representation of 
    the solution, by default defined as a list of objects. 
    There is also an optional integer property called 
    ``score'' used when objects are going to be retrieved in a certain order.
    \item {\bf population\_name/sample/n}
    This is a {\tt GET}  request used to take from the population a 
    sample of {\bf n} objects. These objects are removed from the 
    population and are no longer available
    to other requests until and only if they are put back. 
    Objects can be returned to the population 
    either by a {\tt PUT} sample request called from the same 
    client or by a respawn request. The reason for 
    this is to avoid concurrently write conflicts and duplication of work.
    \item {\bf population\_name/sample}
    This is a {\tt POST} request used to put back a sample to the population.
    The new sample is sent in the request body as a JSON object. 
    If the client created new objects or 
    changed their original state, these objects replace the originals. 
    \item {\bf population\_name/respawn}
    This is a {\tt POST} request used to put back {\bf n} samples to their 
    original state. The number of samples is sent in the request body. 
\end{enumerate}
There are other secondary REST endpoints used to: select all objects in a 
population, select objects with scores with in a range, read the 
top {\tt n} objects according to a score and read the number of
objects currently on the population.      

The above methods were implemented first as JavaScript library 
with two classes: {\tt Individual} and {\tt Population} depicted 
in Figure~\ref{fig:classes} with calls to the Redis memory store %\cite{}
through the {\bf ioredis} asynchronous library. 
% I can describe with more detail the data structures used in Redis
% Set, Lists, Key-Value, etc. - M 

In order to expose the library as a 
REST Web service endpoints were implemented using the Express HTTP framework.
An optional dashboard type application, can be used to inspect 
the populations currently available on the server. This dashboard uses
the Jade (which has recently been renamed to Pugs) template engine and Express.

When a worker is putting back a sample, it can send 
an additional property called {\tt benchmark\_data }
to send supplementary information about the execution 
of the experiment.  This data can later be used to 
benchmark the performance of the algorithm. This 
data is again stored in Redis as an ordered list, 
keeping a log for each experiment. Currently the JSON 
benchmark data structure contains the following details 
to later be used by the COCO platform: the algorithm identifier, 
parameters used, name, dimension, instance and optimal value 
of the function that is been optimized, worker and experiment 
identifiers and finally a list of details of each iteration 
or generation of the local execution. The details include:
the best solution and the function value, and the number of
function evaluations required. Depending on the application
other data could be recorded. The source code for the {\sf evospace-js}
server is in the following Github repository: 
\url{https://github.com/mariosky/evospace-js}.


\subsection{EvoWorkers}
\label{sec:evoworkers}
As we mentioned earlier, EvoWorkers are independent of 
the population repository, and developers can implement them 
in any language that supports HTTP requests. To develop an 
EvoWorker, a programmer could just write the code needed to 
take a sample of the population and use this sample to 
replace the initial population of a local algorithm. 
Then after a certain number of iterations return the 
current population back to the server.

In this work, EvoWorkers were implemented in Python 
taking advantage of two open source libraries of nature inspired optimization 
metaheuristics:  DEAP \cite{fortin2012deap}, which use the
configuration parameters included in Table \ref{tab:DEAP:GAparams} and EvoloPy 
\cite{DBLP:conf/ijcci/FarisAMCM16} using the parameters listed in
Table \ref{tab:PSOparams}. For each algorithm a 
Python script was responsible for the initialization using 
the required parameters and setting up the initial population, 
then after some iterations, the current population and 
benchmark data is sent back to the server. EvoWorker 
scripts can run in Docker containers, by receiving 
the initial parameters as environment variables, and 
the script ends when it reaches a maximum number of samples.
The source code for the Python EvoWorkers
proposed in this work are in the following GitHub repository: 
\url{https://github.com/mariosky/EvoWorker}. 

% Please check that this is correct. Kind of what the reviewer said. - JJ
%Please note that this kind of synchronous implementation might
%introduce some inconsistencies in the data reporting in the beginning
%stages of the data. The reported evaluations are the ones that have
%returned from EvoWorkers, which might have the effect we observe on
%the generated charts.  

\section{Experiments}
 \label{sec:experiments}
As a case study, a simple hybrid algorithm consisting 
of PSO and GA EvoWorkers was used to run a benchmark that 
included the first three functions found in the COCO platform:  
Sphere (F1), Ellipsoid (F2), and Rastrigin (F3). The objective 
of the algorithm is not to be a competitive solution for 
the optimization benchmark, as the intention is to test 
the software functionality. After the 
execution, a script processed the logs and generated the files 
needed by the COCO platform post-processing scripts. 

A requirement of the COCO platform is that it needs 
to inspect each function evaluation to keep the log required 
to analyze the execution. The logging code maintains 
a sequential record of the number function calls. 
This exact order is not practical to keep in an asynchronous 
execution as many workers are calling the function at the 
same time. For this reason, the granularity of the number of 
function evaluations and their order is kept at the 
sample-iteration level. As we mentioned earlier, each worker 
returns a record with benchmark data, with the number of 
evaluations performed in each iteration.

As we mentioned earlier, each worker returns the number 
of evaluations performed in each iteration. The order 
of function calls was given by the order in which the 
server received the samples and the order of the 
iterations in each. On the other hand, the number of 
function evaluations is incremented in each iteration 
by the sample size and the best function evaluation is 
assigned that number,  as if in each iteration the best 
solution was found in the last function evaluation. Instead 
of increasing the number by one it is incremented by the 
number of solutions in the sample. 
Is important to notice that EvoWorkers run the algorithm 
only for a small number of iterations and with 
a relatively small sample of the population. For instance,
for the COCO benchmark presented in the case study the maximum number of 
function evaluations in a single iteration was 200. 
  

\subsection{Set-up}
\label{sec:setup}

A script was responsible for creating the EvoWorker 
containers and running the benchmark. The first three functions 
F1 to F3 were tested with 15 instances for each of 
the dimensions: 2, 3, 5, 10, and 20. 
The maximum number of function evaluations was set to $10^5*dim$. 
In order to maintain the required number of function 
evaluations the following EvoWorker setup was set for
each dimension. An instance of a function in COCO is a different
version of the function, with a different location of the global 
optimum and a different optimal function value.  

\begin{table}
  \small
  \caption{EvoWorker Setup}
  \label{tab:params} 
  \centering
  \small
  \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Dimension & 2 & 3 & 5 & 10 & 20 & 40\\ \hline
    Iterations per Sample  & 50 & 50 & 50 & 50 & 50 & 50\\ \hline
    Sample Size  & 100 & 100 & 100 & 200 & 200 & 200 \\ \hline
    Samples per Worker & 20 & 30 & 25 & 25 & 25 & 25  \\ \hline
    PSO Workers & 1 & 1 & 2 & 2 & 4 & 8  \\ \hline
  \end{tabular}
\end{table}



\begin{table}
  \small
  \caption{ DEAP GA EvoWorker Parameters }
  \label{tab:DEAP:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Search space &  $[-4,4]^{D}$ \\ \hline
    Selection & Tournament size=12\\ \hline
    Mutation & Gaussian $\mu=0.0$, $\sigma=0.5$, indbp=0.05  \\ \hline
    Mutation Probability & [.1,.6]  \\ \hline
    Crossover & Two Point  \\ \hline
    Crossover Probability& [.8,1]  \\ \hline
  \end{tabular}
\end{table}

\begin{table}
  \small
  \caption{ EvoloPy PSO EvoWorker Parameters }
  \label{tab:PSOparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Search space &  $[-4,4]^{D}$ \\ \hline
    $V_{max}$ & 6 \\ \hline
    $W_{max}$ & $0.9$ \\ \hline
    $W_{min}$ & $0.2$ \\ \hline
    $C_1$ & 2 \\ \hline
    $C_2$ & 2 \\ \hline
  \end{tabular}
\end{table}


%TO DO: TABLE
%Data in https://docs.google.com/spreadsheets/d/123s_p6ABtyT2wdifxjuHVGuXjTZECmfzOddxUTydbI8/edit?usp=sharing



\subsection{Results}
\label{sec:results}
\begin{figure}[h!tbp]
    \centering
        \includegraphics[width=3in]{img/f1.pdf}
    \caption{Average numbers of function evaluations to reach the target
      for every dimension for the Sphere function. This figure and the
    following ones have been generated with the BBOB report
    generator. The lines represent the average running times, the crosses
    the median runtime of successful runs to reach the most difficult
    target, x-crosses the max number of f-evaluations in any
    trial. The $y$ scale is logarithmic. The light line on the bottom
    with diamonds indicates the best algorithm from BBOB 2009. This
    shows that, for the time being, the runtime is worse than the one
    obtained in the best BBOB 2009; however, for the time being no
    attempt to optimize results has been made}
  % Does this means our runtime is worse than BBOB 2009? - JJ
  % Yes, at this moment but we have been working on the platform
  % not yet on the optimization. - M
  % Also I hope that this apprach will work better on more difficult
  % functions were more diversity is needed. - M
  % We should say that on the paper
\label{fig:sphere}
\end{figure}
%
After the experiment was run a script generated 
the files and folders needed by the COCO post-processing 
scripts; this script generated the comparative tables and
graphics for checking the performance of the algorithm against
those found in the COCO repository. Results from experiments 
according to \cite{hansen2016coco} on the benchmark functions 
are presented in Figures \ref{fig:sphere}, \ref{fig:rastrigin} and \ref{fig:ellipsoid}.
The whole experiment, with all dimensions and instances, took 
less three hours to execute. %Computer? - JJ
The results obtained show that the 
algorithm performs quite well on separable functions 1-3, 
both regarding results and scaling when compared to results 
from other nature inspired algorithms \cite{hansen2010bbob}. Results
obtained with the Rastrigin function are better than the rest, giving
hope that with adequate tuning much better results could be obtained.
%
\begin{figure}[h!tbp]
    \centering
        \includegraphics[width=3in]{img/f2.pdf}
    \caption{Average numbers of function evaluations to reach target
      for every dimension for the Ellipsoid function.}
    \label{fig:ellipsoid}
\end{figure}
%
\begin{figure}[h!tbp]
    \centering
        \includegraphics[width=3in]{img/f3.pdf}
    \caption{Average numbers of function evaluations to reach target
      for every dimension for the Rastrigin function.}
    \label{fig:rastrigin}
\end{figure}

In fact, all runs have ended successfully with a completely new
asynchronous, heterogeneous, and pool based evolutionary
algorithm. Our intention in this paper was to present this framework
and present how it can successfully tackle and solve difficult
problems using heterogeneous workers that consume the same {\sf
  evospace-js} API. 

\section{Conclusions and Further Work}
\label{sec:conclusions}

The {\sf evospace-js} platform has been applied to implement 
and test a hybrid nature-inspired algorithm against a testbed 
of noiseless continuous functions. Design and implementation 
details have been presented.  The results obtained show that 
an asynchronous execution following a pool-based approach is 
possible and easy to implement, and that it is able to successfully
find solutions to difficult optimization problems. Results, however, are still 
preliminary and further tuning of the parameters could 
potentially yield better results, mainly time-wise. There is also the question about
the performance of the algorithm on the remaining functions
of the benchmark and other real-world problems; however, these are
only implementation details.

Our results have been entirely published in GitHub, along with
sources. We think it is important to help reproductibility as much as
possible by opening our science. Publishing results and software with
an open source license will help achieve this reproductibility, and you
can find all results, code as well as the source for this paper in
(URL HIDDEN FOR DOUBLE-BLIND REASONS).

Future lines of work will focus on using other EA or 
meta-heuristic population-based techniques, such as the Grey Wolf Optimizer \cite{mirjalili2014grey}
or Differential Evolution \cite{storn1997differential} for creating workers that are 
heterogeneous in more than one sense. RPSS could be used 
in those cases where each algorithm has a different set of 
parameters, but also to randomly select the technique employed 
in each node.

Another interesting line of work is the dynamic 
adaptation of parameters by measuring the diversity of each 
worker or returned sample. This adaptation could be especially 
useful in cases where the random parametrization technique 
seems to achieve bad results. 

\begin{acks}
This work has been supported in part by  Ministerio espa\~{n}ol de
Econom\'{\i}a y Competitividad under project TIN2014-56494-C4-3-P
(UGR-EPHEMECH).
\end{acks}
